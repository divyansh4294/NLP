{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenization_stemming_lemmetizing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGumqdWmBxJS"
      },
      "source": [
        "\"\"\"\n",
        "@Author = Divyansh.Gupta\n",
        "\"\"\"\n",
        "import nltk\n",
        "nltk.download()\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_gu4YCZCnrx"
      },
      "source": [
        "# Tokenization\n",
        "Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n",
        "\n",
        "## NLTK\n",
        "It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,\n",
        "along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1-PLXz3C52j"
      },
      "source": [
        "# Simple paraphrase\n",
        "para = \"\"\"Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. This field focuses on how to program computers to process and analyze large amounts of natural language data. It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance. Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\"\"\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "_YFfshr9EJ_7",
        "outputId": "067eed20-3e95-41a9-be99-f8b31f34d6cd"
      },
      "source": [
        "para"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. This field focuses on how to program computers to process and analyze large amounts of natural language data. It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance. Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJn595jYDFzG",
        "outputId": "d5a36374-0f66-4804-ba64-1965e0f55dd3"
      },
      "source": [
        "# Finding sentences using nltk\n",
        "sentences = nltk.sent_tokenize(para)\n",
        "sentences"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction.',\n",
              " 'This field focuses on how to program computers to process and analyze large amounts of natural language data.',\n",
              " 'It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance.',\n",
              " 'Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.',\n",
              " 'One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CneeDX2D9zi",
        "outputId": "d57963eb-f2fd-4de3-b165-99043dd74121"
      },
      "source": [
        "print(\"Total number of sentences in paraphase:\",len(sentences))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of sentences in paraphase: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLr0aeYyFY6o",
        "outputId": "e54fa68d-4b01-4676-876c-4a847256c2a1"
      },
      "source": [
        "# Tokenizing words\n",
        "words = nltk.word_tokenize(para)\n",
        "print(\"Total number of words in paraphase:\",len(words))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in paraphase: 111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCvta2kcBeR0"
      },
      "source": [
        "# Stemming\n",
        "\n",
        "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18rGiOFcBq9f"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords # removing stopwords generally stopwords not make sense in sentence."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2ElKVO3CxaG"
      },
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd6miDxzFI4E",
        "outputId": "7c685988-95d6-4e13-9431-1b65181ce528"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5luwsn7zDCAB"
      },
      "source": [
        "stem_words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww3MmQCSDpw9",
        "outputId": "eaa9b72d-7d47-4326-8e57-6f55c2e27966"
      },
      "source": [
        "print(\"Initially we have {} words in para now after removing stopwords we have {} words\".format(len(words),len(stem_words)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initially we have 111 words in para now after removing stopwords we have 68 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDsU4_TIH4wI"
      },
      "source": [
        "# Lemmetization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaLcU4YvFcUR"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HknoEcaHIApT"
      },
      "source": [
        "lemmetizer = WordNetLemmatizer()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZSAVc9cJK90",
        "outputId": "cc50102c-19a9-4866-e578-014b9edb3958"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_-VCVysIECF"
      },
      "source": [
        "lemm_words = [lemmetizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjv7Dub8IJGe",
        "outputId": "e02539a4-81c7-462b-b248-5614bfb04ee6"
      },
      "source": [
        "print(\"Initially we have {} words in para now after removing stopwords we have {} words\".format(len(words),len(lemm_words)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initially we have 111 words in para now after removing stopwords we have 68 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFzZ6N6LJjBN"
      },
      "source": [
        "now, we are going to see the difference between Stemming and lemmatization.\n",
        "As you can see stemming generate some words which has no meaning while lemmatization has generated meaningful words. So this is the problem with stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RynFOmlVJVxx",
        "outputId": "e29887b8-f8b9-4336-bc1f-b6de5ce189a9"
      },
      "source": [
        "stem_words[:10]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natur',\n",
              " 'languag',\n",
              " 'process',\n",
              " '(',\n",
              " 'nlp',\n",
              " ')',\n",
              " 'subfield',\n",
              " 'comput',\n",
              " 'scienc',\n",
              " ',']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGo0XgvjJZMT",
        "outputId": "94c7eab9-f037-4902-925f-6fd931e68aa1"
      },
      "source": [
        "lemm_words[:10]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'subfield',\n",
              " 'computer',\n",
              " 'science',\n",
              " ',']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqAImpUVJdvd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}